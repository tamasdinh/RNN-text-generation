{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.getcwd() + '/data/anna.txt') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating integer coding for characters\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {char: ii for ii, char in int2char.items()}\n",
    "\n",
    "# encoding whole text\n",
    "encoded = np.array([char2int[char] for char in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(arr, n_labels):\n",
    "    \"\"\"\n",
    "    :param arr: numpy array containing elements encoded to integers\n",
    "    :param n_labels: total size of dictionary\n",
    "    :return: one-hot encoded array\n",
    "    \"\"\"\n",
    "\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)  # initialize properly sized array with 0s\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1  # fill appropriate positions with 1s\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))  # reshape array to original dimensions\n",
    "\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_ex = np.array([[3, 5, 1]])\n",
    "num_labels = 8\n",
    "one_hot_encoding(array_ex, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    :param arr: numpy array containing elements encoded to integers\n",
    "    :param batch_size: number of data sequences in one batch\n",
    "    :param seq_length: length of sequence in batches\n",
    "    :return: one batch per function call (generator) for training data (x) and target (y)\n",
    "    \"\"\"\n",
    "    n_batches = arr.shape[0] // (batch_size * seq_length)\n",
    "    arr = arr[:batch_size * n_batches * seq_length]\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        y = np.zeros_like(x)  # this is to avoid y going over the boundaries of arr at the last batch\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[66 72 77 42 10 74 12 31 43  7  7  7 48 77 42 42 73 31 79 77 44 63  0 63\n",
      "  74 49 31 77 12 74 31 77  0  0 31 77  0 63 52 74  4 31 74 40 74 12 73 31\n",
      "   8 16]\n",
      " [49 37 16 31 10 72 77 10 31 77 10 10 12 77 76 10 74 11 31 72 74 12 31 77\n",
      "  10 10 74 16 10 63 37 16 31 28 77 49 31 72 74 12 31 72  8 49 15 77 16 11\n",
      "  71 31]\n",
      " [74 16 11 31 37 12 31 77 31 79 37 74 60 31 72 74 31 77 40 37 63 11 74 11\n",
      "  31 72 63 49 31 79 77 10 72 74 12 71 31 48 74  7  0 37 37 52 74 11 31 12\n",
      "  37  8]\n",
      " [49 31 10 72 74 31 76 72 63 74 79 31 10 72 37  8 19 72 31 72 63 11 11 74\n",
      "  16  7 63 16 10 74 12 74 49 10 31 37 79 31 72 63 49 31  0 63 79 74 60 31\n",
      "  37 79]\n",
      " [31 49 77 28 31 72 74 12 31 10 74 77 12 69 49 10 77 63 16 74 11 60 31 42\n",
      "  63 10 63 79  8  0 60 31 49 28 74 74 10 31 79 77 76 74 60  7 44 63 49 74\n",
      "  12 77]\n",
      " [76  8 49 49 63 37 16 31 77 16 11 31 77 16 77  0 73 49 63 49 60 31 28 77\n",
      "  49 31 63 16 31 42 12 63 16 76 63 42  0 74 31 11 63 49 77 19 12 74 74 77\n",
      "  15  0]\n",
      " [31 75 16 16 77 31 72 77 11 31 49 77 63 11 31 10 72 77 10 31 14 37  0  0\n",
      "  73 31 28 37  8  0 11 31 74 36 76  8 49 74 31 63 10 71 31 75 16 11 31 10\n",
      "  72 63]\n",
      " [20 15  0 37 16 49 52 73 71 31 62 55  8 10 31 45 10 72 74 73 45 31 76 77\n",
      "  16 16 37 10 31 19 12 77 49 42 31 10 72 77 10 60  7 45 10 72 74 73 45 31\n",
      "  77 12]]\n",
      "y\n",
      " [[72 77 42 10 74 12 31 43  7  7  7 48 77 42 42 73 31 79 77 44 63  0 63 74\n",
      "  49 31 77 12 74 31 77  0  0 31 77  0 63 52 74  4 31 74 40 74 12 73 31  8\n",
      "  16 72]\n",
      " [37 16 31 10 72 77 10 31 77 10 10 12 77 76 10 74 11 31 72 74 12 31 77 10\n",
      "  10 74 16 10 63 37 16 31 28 77 49 31 72 74 12 31 72  8 49 15 77 16 11 71\n",
      "  31 62]\n",
      " [16 11 31 37 12 31 77 31 79 37 74 60 31 72 74 31 77 40 37 63 11 74 11 31\n",
      "  72 63 49 31 79 77 10 72 74 12 71 31 48 74  7  0 37 37 52 74 11 31 12 37\n",
      "   8 16]\n",
      " [31 10 72 74 31 76 72 63 74 79 31 10 72 37  8 19 72 31 72 63 11 11 74 16\n",
      "   7 63 16 10 74 12 74 49 10 31 37 79 31 72 63 49 31  0 63 79 74 60 31 37\n",
      "  79 31]\n",
      " [49 77 28 31 72 74 12 31 10 74 77 12 69 49 10 77 63 16 74 11 60 31 42 63\n",
      "  10 63 79  8  0 60 31 49 28 74 74 10 31 79 77 76 74 60  7 44 63 49 74 12\n",
      "  77 15]\n",
      " [ 8 49 49 63 37 16 31 77 16 11 31 77 16 77  0 73 49 63 49 60 31 28 77 49\n",
      "  31 63 16 31 42 12 63 16 76 63 42  0 74 31 11 63 49 77 19 12 74 74 77 15\n",
      "   0 74]\n",
      " [75 16 16 77 31 72 77 11 31 49 77 63 11 31 10 72 77 10 31 14 37  0  0 73\n",
      "  31 28 37  8  0 11 31 74 36 76  8 49 74 31 63 10 71 31 75 16 11 31 10 72\n",
      "  63 49]\n",
      " [15  0 37 16 49 52 73 71 31 62 55  8 10 31 45 10 72 74 73 45 31 76 77 16\n",
      "  16 37 10 31 19 12 77 49 42 31 10 72 77 10 60  7 45 10 72 74 73 45 31 77\n",
      "  12 74]]\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(encoded, 8, 50)\n",
    "x, y = next(batches)\n",
    "print('x\\n', x)\n",
    "print('y\\n', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "if train_on_gpu:\n",
    "    print('Training on GPU')\n",
    "else:\n",
    "    print('No GPU available; training on CPU. Expect LONG runtimes - or keep the number of epochs limited.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {char: ii for ii, char in self.int2char.items()}\n",
    "\n",
    "        self.lstm = nn.LSTM(len(self.chars), self.n_hidden,\n",
    "                            self.n_layers, dropout=self.drop_prob, batch_first=True)\n",
    "        # here dropout automatically creates dropout layer between LSTM cells\n",
    "\n",
    "        self.dropout = nn.Dropout(p=self.drop_prob)  # we need this for dropout bw LSTM and final FC layer\n",
    "\n",
    "        self.fc = nn.Linear(in_features=self.n_hidden, out_features=len(self.chars))\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        :param x: input sequence of characters (one-hot encoded)\n",
    "        :param hidden: values of hidden layer\n",
    "        :return: final output, hidden state\n",
    "        \"\"\"\n",
    "        lstm_out, hidden_state = self.lstm(x, hidden)\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden_state\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Initializes hidden state.\n",
    "        :param batch_size: batch size\n",
    "        :return: hidden state\n",
    "        \"\"\"\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        if train_on_gpu:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    \"\"\"\n",
    "    Defines the process for training the network\n",
    "    :param net: CharRNN network\n",
    "    :param data: text data to train the network\n",
    "    :param epochs: number of epochs to run\n",
    "    :param batch_size: number of data records in a batch\n",
    "    :param seq_length: number of characters in one line of a mini-batch\n",
    "    :param lr: learning rate\n",
    "    :param clip: limit for gradient clipping\n",
    "    :param val_frac: fraction of validation set compared to total size of input data\n",
    "    :param print_every: number of steps by which loss is printed to console\n",
    "    :return: none\n",
    "    \"\"\"\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "\n",
    "    if train_on_gpu:\n",
    "        net.cuda()\n",
    "\n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        h = net.init_hidden(batch_size)\n",
    "\n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "\n",
    "            x = one_hot_encoding(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "            if train_on_gpu:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            net.zero_grad()\n",
    "\n",
    "            output, h = net(inputs, h)\n",
    "\n",
    "            loss = criterion(output, targets.contiguous().view(batch_size*seq_length))\n",
    "            loss.backward()\n",
    "\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "\n",
    "            opt.step()\n",
    "\n",
    "            if counter % print_every == 0:\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    x = one_hot_encoding(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                    inputs, targets = x, y\n",
    "                    if train_on_gpu:\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "\n",
    "                    val_loss = criterion(output, targets.contiguous().view(batch_size*seq_length))\n",
    "\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "                net.train()\n",
    "\n",
    "                print('Epoch: {}/{}...'.format(e+1, epochs),\n",
    "                      'Step: {}...'.format(counter),\n",
    "                      'Loss: {:.4f}...'.format(loss.item()),\n",
    "                      'Val loss: {:.4f}...'.format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 512\n",
    "n_layers = 2\n",
    "\n",
    "net = CharRNN(tokens=chars, n_hidden=n_hidden, n_layers=n_layers)\n",
    "print(net)\n",
    "#for i in net.named_parameters():\n",
    "#    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 10... Loss: 3.2385... Val loss: 3.1865...\n",
      "Epoch: 1/20... Step: 20... Loss: 3.1430... Val loss: 3.1264...\n",
      "Epoch: 1/20... Step: 30... Loss: 3.1368... Val loss: 3.1205...\n",
      "Epoch: 1/20... Step: 40... Loss: 3.1114... Val loss: 3.1190...\n",
      "Epoch: 1/20... Step: 50... Loss: 3.1399... Val loss: 3.1165...\n",
      "Epoch: 1/20... Step: 60... Loss: 3.1141... Val loss: 3.1140...\n",
      "Epoch: 1/20... Step: 70... Loss: 3.1078... Val loss: 3.1115...\n",
      "Epoch: 1/20... Step: 80... Loss: 3.1139... Val loss: 3.1026...\n",
      "Epoch: 1/20... Step: 90... Loss: 3.1021... Val loss: 3.0866...\n",
      "Epoch: 1/20... Step: 100... Loss: 3.0546... Val loss: 3.0382...\n",
      "Epoch: 1/20... Step: 110... Loss: 2.9764... Val loss: 2.9449...\n",
      "Epoch: 1/20... Step: 120... Loss: 2.7943... Val loss: 2.7839...\n",
      "Epoch: 1/20... Step: 130... Loss: 2.7598... Val loss: 2.7075...\n",
      "Epoch: 2/20... Step: 140... Loss: 2.6421... Val loss: 2.5907...\n",
      "Epoch: 2/20... Step: 150... Loss: 2.5803... Val loss: 2.5272...\n",
      "Epoch: 2/20... Step: 160... Loss: 2.5251... Val loss: 2.4870...\n",
      "Epoch: 2/20... Step: 170... Loss: 2.4575... Val loss: 2.4421...\n",
      "Epoch: 2/20... Step: 180... Loss: 2.4371... Val loss: 2.4206...\n",
      "Epoch: 2/20... Step: 190... Loss: 2.3791... Val loss: 2.3803...\n",
      "Epoch: 2/20... Step: 200... Loss: 2.3852... Val loss: 2.3626...\n",
      "Epoch: 2/20... Step: 210... Loss: 2.3474... Val loss: 2.3332...\n",
      "Epoch: 2/20... Step: 220... Loss: 2.3101... Val loss: 2.2997...\n",
      "Epoch: 2/20... Step: 230... Loss: 2.2991... Val loss: 2.2702...\n",
      "Epoch: 2/20... Step: 240... Loss: 2.2802... Val loss: 2.2439...\n",
      "Epoch: 2/20... Step: 250... Loss: 2.2109... Val loss: 2.2183...\n",
      "Epoch: 2/20... Step: 260... Loss: 2.1895... Val loss: 2.1901...\n",
      "Epoch: 2/20... Step: 270... Loss: 2.1964... Val loss: 2.1712...\n",
      "Epoch: 3/20... Step: 280... Loss: 2.1927... Val loss: 2.1561...\n",
      "Epoch: 3/20... Step: 290... Loss: 2.1570... Val loss: 2.1276...\n",
      "Epoch: 3/20... Step: 300... Loss: 2.1333... Val loss: 2.1033...\n",
      "Epoch: 3/20... Step: 310... Loss: 2.0953... Val loss: 2.0859...\n",
      "Epoch: 3/20... Step: 320... Loss: 2.0650... Val loss: 2.0624...\n",
      "Epoch: 3/20... Step: 330... Loss: 2.0514... Val loss: 2.0481...\n",
      "Epoch: 3/20... Step: 340... Loss: 2.0673... Val loss: 2.0355...\n",
      "Epoch: 3/20... Step: 350... Loss: 2.0451... Val loss: 2.0107...\n",
      "Epoch: 3/20... Step: 360... Loss: 1.9815... Val loss: 1.9917...\n",
      "Epoch: 3/20... Step: 370... Loss: 1.9995... Val loss: 1.9752...\n",
      "Epoch: 3/20... Step: 380... Loss: 1.9748... Val loss: 1.9548...\n",
      "Epoch: 3/20... Step: 390... Loss: 1.9534... Val loss: 1.9431...\n",
      "Epoch: 3/20... Step: 400... Loss: 1.9195... Val loss: 1.9258...\n",
      "Epoch: 3/20... Step: 410... Loss: 1.9312... Val loss: 1.9095...\n",
      "Epoch: 4/20... Step: 420... Loss: 1.9276... Val loss: 1.9008...\n",
      "Epoch: 4/20... Step: 430... Loss: 1.9031... Val loss: 1.8789...\n",
      "Epoch: 4/20... Step: 440... Loss: 1.8917... Val loss: 1.8687...\n",
      "Epoch: 4/20... Step: 450... Loss: 1.8372... Val loss: 1.8542...\n",
      "Epoch: 4/20... Step: 460... Loss: 1.8324... Val loss: 1.8469...\n",
      "Epoch: 4/20... Step: 470... Loss: 1.8667... Val loss: 1.8346...\n",
      "Epoch: 4/20... Step: 480... Loss: 1.8390... Val loss: 1.8187...\n",
      "Epoch: 4/20... Step: 490... Loss: 1.8359... Val loss: 1.8056...\n",
      "Epoch: 4/20... Step: 500... Loss: 1.8232... Val loss: 1.7932...\n",
      "Epoch: 4/20... Step: 510... Loss: 1.8057... Val loss: 1.7818...\n",
      "Epoch: 4/20... Step: 520... Loss: 1.8175... Val loss: 1.7718...\n",
      "Epoch: 4/20... Step: 530... Loss: 1.7743... Val loss: 1.7644...\n",
      "Epoch: 4/20... Step: 540... Loss: 1.7460... Val loss: 1.7587...\n",
      "Epoch: 4/20... Step: 550... Loss: 1.7874... Val loss: 1.7430...\n",
      "Epoch: 5/20... Step: 560... Loss: 1.7534... Val loss: 1.7351...\n",
      "Epoch: 5/20... Step: 570... Loss: 1.7406... Val loss: 1.7222...\n",
      "Epoch: 5/20... Step: 580... Loss: 1.7144... Val loss: 1.7126...\n",
      "Epoch: 5/20... Step: 590... Loss: 1.7201... Val loss: 1.7043...\n",
      "Epoch: 5/20... Step: 600... Loss: 1.7073... Val loss: 1.6968...\n",
      "Epoch: 5/20... Step: 610... Loss: 1.6857... Val loss: 1.6951...\n",
      "Epoch: 5/20... Step: 620... Loss: 1.6917... Val loss: 1.6842...\n",
      "Epoch: 5/20... Step: 630... Loss: 1.7082... Val loss: 1.6761...\n",
      "Epoch: 5/20... Step: 640... Loss: 1.6815... Val loss: 1.6671...\n",
      "Epoch: 5/20... Step: 650... Loss: 1.6708... Val loss: 1.6607...\n",
      "Epoch: 5/20... Step: 660... Loss: 1.6423... Val loss: 1.6584...\n",
      "Epoch: 5/20... Step: 670... Loss: 1.6688... Val loss: 1.6499...\n",
      "Epoch: 5/20... Step: 680... Loss: 1.6597... Val loss: 1.6394...\n",
      "Epoch: 5/20... Step: 690... Loss: 1.6373... Val loss: 1.6352...\n",
      "Epoch: 6/20... Step: 700... Loss: 1.6454... Val loss: 1.6322...\n",
      "Epoch: 6/20... Step: 710... Loss: 1.6245... Val loss: 1.6234...\n",
      "Epoch: 6/20... Step: 720... Loss: 1.6199... Val loss: 1.6179...\n",
      "Epoch: 6/20... Step: 730... Loss: 1.6267... Val loss: 1.6104...\n",
      "Epoch: 6/20... Step: 740... Loss: 1.5946... Val loss: 1.6050...\n",
      "Epoch: 6/20... Step: 750... Loss: 1.5807... Val loss: 1.6009...\n",
      "Epoch: 6/20... Step: 760... Loss: 1.6285... Val loss: 1.5994...\n",
      "Epoch: 6/20... Step: 770... Loss: 1.5970... Val loss: 1.5960...\n",
      "Epoch: 6/20... Step: 780... Loss: 1.5818... Val loss: 1.5874...\n",
      "Epoch: 6/20... Step: 790... Loss: 1.5689... Val loss: 1.5780...\n",
      "Epoch: 6/20... Step: 800... Loss: 1.5889... Val loss: 1.5751...\n",
      "Epoch: 6/20... Step: 810... Loss: 1.5743... Val loss: 1.5735...\n",
      "Epoch: 6/20... Step: 820... Loss: 1.5373... Val loss: 1.5673...\n",
      "Epoch: 6/20... Step: 830... Loss: 1.5923... Val loss: 1.5611...\n",
      "Epoch: 7/20... Step: 840... Loss: 1.5404... Val loss: 1.5632...\n",
      "Epoch: 7/20... Step: 850... Loss: 1.5657... Val loss: 1.5551...\n",
      "Epoch: 7/20... Step: 860... Loss: 1.5362... Val loss: 1.5448...\n",
      "Epoch: 7/20... Step: 870... Loss: 1.5474... Val loss: 1.5396...\n",
      "Epoch: 7/20... Step: 880... Loss: 1.5502... Val loss: 1.5423...\n",
      "Epoch: 7/20... Step: 890... Loss: 1.5580... Val loss: 1.5388...\n",
      "Epoch: 7/20... Step: 900... Loss: 1.5333... Val loss: 1.5354...\n",
      "Epoch: 7/20... Step: 910... Loss: 1.4993... Val loss: 1.5325...\n",
      "Epoch: 7/20... Step: 920... Loss: 1.5349... Val loss: 1.5253...\n",
      "Epoch: 7/20... Step: 930... Loss: 1.5130... Val loss: 1.5209...\n",
      "Epoch: 7/20... Step: 940... Loss: 1.5158... Val loss: 1.5186...\n",
      "Epoch: 7/20... Step: 950... Loss: 1.5317... Val loss: 1.5169...\n",
      "Epoch: 7/20... Step: 960... Loss: 1.5156... Val loss: 1.5092...\n",
      "Epoch: 7/20... Step: 970... Loss: 1.5256... Val loss: 1.5055...\n",
      "Epoch: 8/20... Step: 980... Loss: 1.4968... Val loss: 1.5031...\n",
      "Epoch: 8/20... Step: 990... Loss: 1.5005... Val loss: 1.5002...\n",
      "Epoch: 8/20... Step: 1000... Loss: 1.4841... Val loss: 1.4958...\n",
      "Epoch: 8/20... Step: 1010... Loss: 1.5307... Val loss: 1.4952...\n",
      "Epoch: 8/20... Step: 1020... Loss: 1.5027... Val loss: 1.4921...\n",
      "Epoch: 8/20... Step: 1030... Loss: 1.4820... Val loss: 1.4945...\n",
      "Epoch: 8/20... Step: 1040... Loss: 1.4947... Val loss: 1.4896...\n",
      "Epoch: 8/20... Step: 1050... Loss: 1.4772... Val loss: 1.4843...\n",
      "Epoch: 8/20... Step: 1060... Loss: 1.4843... Val loss: 1.4821...\n",
      "Epoch: 8/20... Step: 1070... Loss: 1.4905... Val loss: 1.4812...\n",
      "Epoch: 8/20... Step: 1080... Loss: 1.4844... Val loss: 1.4766...\n",
      "Epoch: 8/20... Step: 1090... Loss: 1.4641... Val loss: 1.4716...\n",
      "Epoch: 8/20... Step: 1100... Loss: 1.4630... Val loss: 1.4685...\n",
      "Epoch: 8/20... Step: 1110... Loss: 1.4592... Val loss: 1.4647...\n",
      "Epoch: 9/20... Step: 1120... Loss: 1.4772... Val loss: 1.4661...\n",
      "Epoch: 9/20... Step: 1130... Loss: 1.4752... Val loss: 1.4601...\n",
      "Epoch: 9/20... Step: 1140... Loss: 1.4687... Val loss: 1.4549...\n",
      "Epoch: 9/20... Step: 1150... Loss: 1.4813... Val loss: 1.4555...\n",
      "Epoch: 9/20... Step: 1160... Loss: 1.4302... Val loss: 1.4536...\n",
      "Epoch: 9/20... Step: 1170... Loss: 1.4430... Val loss: 1.4518...\n",
      "Epoch: 9/20... Step: 1180... Loss: 1.4377... Val loss: 1.4550...\n",
      "Epoch: 9/20... Step: 1190... Loss: 1.4723... Val loss: 1.4475...\n",
      "Epoch: 9/20... Step: 1200... Loss: 1.4245... Val loss: 1.4449...\n",
      "Epoch: 9/20... Step: 1210... Loss: 1.4319... Val loss: 1.4418...\n",
      "Epoch: 9/20... Step: 1220... Loss: 1.4302... Val loss: 1.4431...\n",
      "Epoch: 9/20... Step: 1230... Loss: 1.4100... Val loss: 1.4378...\n",
      "Epoch: 9/20... Step: 1240... Loss: 1.4153... Val loss: 1.4371...\n",
      "Epoch: 9/20... Step: 1250... Loss: 1.4355... Val loss: 1.4355...\n",
      "Epoch: 10/20... Step: 1260... Loss: 1.4378... Val loss: 1.4311...\n",
      "Epoch: 10/20... Step: 1270... Loss: 1.4244... Val loss: 1.4317...\n",
      "Epoch: 10/20... Step: 1280... Loss: 1.4327... Val loss: 1.4246...\n",
      "Epoch: 10/20... Step: 1290... Loss: 1.4210... Val loss: 1.4251...\n",
      "Epoch: 10/20... Step: 1300... Loss: 1.4173... Val loss: 1.4230...\n",
      "Epoch: 10/20... Step: 1310... Loss: 1.4227... Val loss: 1.4238...\n",
      "Epoch: 10/20... Step: 1320... Loss: 1.3886... Val loss: 1.4246...\n",
      "Epoch: 10/20... Step: 1330... Loss: 1.4013... Val loss: 1.4216...\n",
      "Epoch: 10/20... Step: 1340... Loss: 1.3882... Val loss: 1.4143...\n",
      "Epoch: 10/20... Step: 1350... Loss: 1.3831... Val loss: 1.4123...\n",
      "Epoch: 10/20... Step: 1360... Loss: 1.3853... Val loss: 1.4144...\n",
      "Epoch: 10/20... Step: 1370... Loss: 1.3749... Val loss: 1.4161...\n",
      "Epoch: 10/20... Step: 1380... Loss: 1.4175... Val loss: 1.4074...\n",
      "Epoch: 10/20... Step: 1390... Loss: 1.4257... Val loss: 1.4068...\n",
      "Epoch: 11/20... Step: 1400... Loss: 1.4225... Val loss: 1.4091...\n",
      "Epoch: 11/20... Step: 1410... Loss: 1.4386... Val loss: 1.4059...\n",
      "Epoch: 11/20... Step: 1420... Loss: 1.4238... Val loss: 1.3999...\n",
      "Epoch: 11/20... Step: 1430... Loss: 1.3856... Val loss: 1.4034...\n",
      "Epoch: 11/20... Step: 1440... Loss: 1.4129... Val loss: 1.4004...\n",
      "Epoch: 11/20... Step: 1450... Loss: 1.3484... Val loss: 1.3997...\n",
      "Epoch: 11/20... Step: 1460... Loss: 1.3656... Val loss: 1.4007...\n",
      "Epoch: 11/20... Step: 1470... Loss: 1.3639... Val loss: 1.3977...\n",
      "Epoch: 11/20... Step: 1480... Loss: 1.3842... Val loss: 1.3938...\n",
      "Epoch: 11/20... Step: 1490... Loss: 1.3742... Val loss: 1.3932...\n",
      "Epoch: 11/20... Step: 1500... Loss: 1.3539... Val loss: 1.3982...\n",
      "Epoch: 11/20... Step: 1510... Loss: 1.3402... Val loss: 1.3952...\n",
      "Epoch: 11/20... Step: 1520... Loss: 1.3795... Val loss: 1.3882...\n",
      "Epoch: 12/20... Step: 1530... Loss: 1.4346... Val loss: 1.3884...\n",
      "Epoch: 12/20... Step: 1540... Loss: 1.3778... Val loss: 1.3871...\n",
      "Epoch: 12/20... Step: 1550... Loss: 1.3859... Val loss: 1.3838...\n",
      "Epoch: 12/20... Step: 1560... Loss: 1.3929... Val loss: 1.3805...\n",
      "Epoch: 12/20... Step: 1570... Loss: 1.3415... Val loss: 1.3830...\n",
      "Epoch: 12/20... Step: 1580... Loss: 1.3275... Val loss: 1.3817...\n",
      "Epoch: 12/20... Step: 1590... Loss: 1.3193... Val loss: 1.3832...\n",
      "Epoch: 12/20... Step: 1600... Loss: 1.3491... Val loss: 1.3809...\n",
      "Epoch: 12/20... Step: 1610... Loss: 1.3398... Val loss: 1.3792...\n",
      "Epoch: 12/20... Step: 1620... Loss: 1.3505... Val loss: 1.3767...\n",
      "Epoch: 12/20... Step: 1630... Loss: 1.3559... Val loss: 1.3778...\n",
      "Epoch: 12/20... Step: 1640... Loss: 1.3380... Val loss: 1.3813...\n",
      "Epoch: 12/20... Step: 1650... Loss: 1.3203... Val loss: 1.3743...\n",
      "Epoch: 12/20... Step: 1660... Loss: 1.3713... Val loss: 1.3723...\n",
      "Epoch: 13/20... Step: 1670... Loss: 1.3430... Val loss: 1.3703...\n",
      "Epoch: 13/20... Step: 1680... Loss: 1.3508... Val loss: 1.3679...\n",
      "Epoch: 13/20... Step: 1690... Loss: 1.3303... Val loss: 1.3695...\n",
      "Epoch: 13/20... Step: 1700... Loss: 1.3376... Val loss: 1.3648...\n",
      "Epoch: 13/20... Step: 1710... Loss: 1.3033... Val loss: 1.3679...\n",
      "Epoch: 13/20... Step: 1720... Loss: 1.3236... Val loss: 1.3701...\n",
      "Epoch: 13/20... Step: 1730... Loss: 1.3662... Val loss: 1.3671...\n",
      "Epoch: 13/20... Step: 1740... Loss: 1.3302... Val loss: 1.3653...\n",
      "Epoch: 13/20... Step: 1750... Loss: 1.2958... Val loss: 1.3705...\n",
      "Epoch: 13/20... Step: 1760... Loss: 1.3232... Val loss: 1.3648...\n",
      "Epoch: 13/20... Step: 1770... Loss: 1.3427... Val loss: 1.3590...\n",
      "Epoch: 13/20... Step: 1780... Loss: 1.3141... Val loss: 1.3617...\n",
      "Epoch: 13/20... Step: 1790... Loss: 1.3015... Val loss: 1.3616...\n",
      "Epoch: 13/20... Step: 1800... Loss: 1.3346... Val loss: 1.3589...\n",
      "Epoch: 14/20... Step: 1810... Loss: 1.3343... Val loss: 1.3580...\n",
      "Epoch: 14/20... Step: 1820... Loss: 1.3175... Val loss: 1.3569...\n",
      "Epoch: 14/20... Step: 1830... Loss: 1.3383... Val loss: 1.3547...\n",
      "Epoch: 14/20... Step: 1840... Loss: 1.2844... Val loss: 1.3548...\n",
      "Epoch: 14/20... Step: 1850... Loss: 1.2663... Val loss: 1.3547...\n",
      "Epoch: 14/20... Step: 1860... Loss: 1.3206... Val loss: 1.3568...\n",
      "Epoch: 14/20... Step: 1870... Loss: 1.3322... Val loss: 1.3504...\n",
      "Epoch: 14/20... Step: 1880... Loss: 1.3201... Val loss: 1.3546...\n",
      "Epoch: 14/20... Step: 1890... Loss: 1.3372... Val loss: 1.3567...\n",
      "Epoch: 14/20... Step: 1900... Loss: 1.3164... Val loss: 1.3524...\n",
      "Epoch: 14/20... Step: 1910... Loss: 1.3209... Val loss: 1.3487...\n",
      "Epoch: 14/20... Step: 1920... Loss: 1.3068... Val loss: 1.3516...\n",
      "Epoch: 14/20... Step: 1930... Loss: 1.2827... Val loss: 1.3484...\n",
      "Epoch: 14/20... Step: 1940... Loss: 1.3438... Val loss: 1.3472...\n",
      "Epoch: 15/20... Step: 1950... Loss: 1.3005... Val loss: 1.3540...\n",
      "Epoch: 15/20... Step: 1960... Loss: 1.3055... Val loss: 1.3444...\n",
      "Epoch: 15/20... Step: 1970... Loss: 1.3005... Val loss: 1.3470...\n",
      "Epoch: 15/20... Step: 1980... Loss: 1.2901... Val loss: 1.3454...\n",
      "Epoch: 15/20... Step: 1990... Loss: 1.2887... Val loss: 1.3439...\n",
      "Epoch: 15/20... Step: 2000... Loss: 1.2715... Val loss: 1.3466...\n",
      "Epoch: 15/20... Step: 2010... Loss: 1.2979... Val loss: 1.3417...\n",
      "Epoch: 15/20... Step: 2020... Loss: 1.3177... Val loss: 1.3437...\n",
      "Epoch: 15/20... Step: 2030... Loss: 1.2792... Val loss: 1.3428...\n",
      "Epoch: 15/20... Step: 2040... Loss: 1.3101... Val loss: 1.3413...\n",
      "Epoch: 15/20... Step: 2050... Loss: 1.2881... Val loss: 1.3416...\n",
      "Epoch: 15/20... Step: 2060... Loss: 1.2927... Val loss: 1.3405...\n",
      "Epoch: 15/20... Step: 2070... Loss: 1.2949... Val loss: 1.3346...\n",
      "Epoch: 15/20... Step: 2080... Loss: 1.3009... Val loss: 1.3357...\n",
      "Epoch: 16/20... Step: 2090... Loss: 1.2980... Val loss: 1.3399...\n",
      "Epoch: 16/20... Step: 2100... Loss: 1.2915... Val loss: 1.3357...\n",
      "Epoch: 16/20... Step: 2110... Loss: 1.2829... Val loss: 1.3334...\n",
      "Epoch: 16/20... Step: 2120... Loss: 1.2942... Val loss: 1.3322...\n",
      "Epoch: 16/20... Step: 2130... Loss: 1.2663... Val loss: 1.3330...\n",
      "Epoch: 16/20... Step: 2140... Loss: 1.2840... Val loss: 1.3346...\n",
      "Epoch: 16/20... Step: 2150... Loss: 1.2970... Val loss: 1.3300...\n",
      "Epoch: 16/20... Step: 2160... Loss: 1.2767... Val loss: 1.3340...\n",
      "Epoch: 16/20... Step: 2170... Loss: 1.2800... Val loss: 1.3344...\n",
      "Epoch: 16/20... Step: 2180... Loss: 1.2705... Val loss: 1.3341...\n",
      "Epoch: 16/20... Step: 2190... Loss: 1.2919... Val loss: 1.3300...\n",
      "Epoch: 16/20... Step: 2200... Loss: 1.2746... Val loss: 1.3318...\n",
      "Epoch: 16/20... Step: 2210... Loss: 1.2352... Val loss: 1.3284...\n",
      "Epoch: 16/20... Step: 2220... Loss: 1.2810... Val loss: 1.3299...\n",
      "Epoch: 17/20... Step: 2230... Loss: 1.2603... Val loss: 1.3246...\n",
      "Epoch: 17/20... Step: 2240... Loss: 1.2600... Val loss: 1.3251...\n",
      "Epoch: 17/20... Step: 2250... Loss: 1.2583... Val loss: 1.3266...\n",
      "Epoch: 17/20... Step: 2260... Loss: 1.2663... Val loss: 1.3269...\n",
      "Epoch: 17/20... Step: 2270... Loss: 1.2753... Val loss: 1.3270...\n",
      "Epoch: 17/20... Step: 2280... Loss: 1.2757... Val loss: 1.3243...\n",
      "Epoch: 17/20... Step: 2290... Loss: 1.2795... Val loss: 1.3221...\n",
      "Epoch: 17/20... Step: 2300... Loss: 1.2433... Val loss: 1.3239...\n",
      "Epoch: 17/20... Step: 2310... Loss: 1.2628... Val loss: 1.3198...\n",
      "Epoch: 17/20... Step: 2320... Loss: 1.2557... Val loss: 1.3217...\n",
      "Epoch: 17/20... Step: 2330... Loss: 1.2474... Val loss: 1.3207...\n",
      "Epoch: 17/20... Step: 2340... Loss: 1.2736... Val loss: 1.3220...\n",
      "Epoch: 17/20... Step: 2350... Loss: 1.2699... Val loss: 1.3177...\n",
      "Epoch: 17/20... Step: 2360... Loss: 1.2767... Val loss: 1.3225...\n",
      "Epoch: 18/20... Step: 2370... Loss: 1.2492... Val loss: 1.3200...\n",
      "Epoch: 18/20... Step: 2380... Loss: 1.2517... Val loss: 1.3187...\n",
      "Epoch: 18/20... Step: 2390... Loss: 1.2504... Val loss: 1.3200...\n",
      "Epoch: 18/20... Step: 2400... Loss: 1.2838... Val loss: 1.3204...\n",
      "Epoch: 18/20... Step: 2410... Loss: 1.2794... Val loss: 1.3196...\n",
      "Epoch: 18/20... Step: 2420... Loss: 1.2500... Val loss: 1.3143...\n",
      "Epoch: 18/20... Step: 2430... Loss: 1.2518... Val loss: 1.3147...\n",
      "Epoch: 18/20... Step: 2440... Loss: 1.2431... Val loss: 1.3157...\n",
      "Epoch: 18/20... Step: 2450... Loss: 1.2353... Val loss: 1.3115...\n",
      "Epoch: 18/20... Step: 2460... Loss: 1.2574... Val loss: 1.3123...\n",
      "Epoch: 18/20... Step: 2470... Loss: 1.2506... Val loss: 1.3133...\n",
      "Epoch: 18/20... Step: 2480... Loss: 1.2360... Val loss: 1.3138...\n",
      "Epoch: 18/20... Step: 2490... Loss: 1.2411... Val loss: 1.3121...\n",
      "Epoch: 18/20... Step: 2500... Loss: 1.2401... Val loss: 1.3099...\n",
      "Epoch: 19/20... Step: 2510... Loss: 1.2488... Val loss: 1.3070...\n",
      "Epoch: 19/20... Step: 2520... Loss: 1.2599... Val loss: 1.3104...\n",
      "Epoch: 19/20... Step: 2530... Loss: 1.2599... Val loss: 1.3094...\n",
      "Epoch: 19/20... Step: 2540... Loss: 1.2780... Val loss: 1.3072...\n",
      "Epoch: 19/20... Step: 2550... Loss: 1.2273... Val loss: 1.3124...\n",
      "Epoch: 19/20... Step: 2560... Loss: 1.2471... Val loss: 1.3084...\n",
      "Epoch: 19/20... Step: 2570... Loss: 1.2395... Val loss: 1.3093...\n",
      "Epoch: 19/20... Step: 2580... Loss: 1.2808... Val loss: 1.3083...\n",
      "Epoch: 19/20... Step: 2590... Loss: 1.2415... Val loss: 1.3059...\n",
      "Epoch: 19/20... Step: 2600... Loss: 1.2380... Val loss: 1.3038...\n",
      "Epoch: 19/20... Step: 2610... Loss: 1.2432... Val loss: 1.3081...\n",
      "Epoch: 19/20... Step: 2620... Loss: 1.2287... Val loss: 1.3074...\n",
      "Epoch: 19/20... Step: 2630... Loss: 1.2326... Val loss: 1.3032...\n",
      "Epoch: 19/20... Step: 2640... Loss: 1.2454... Val loss: 1.3025...\n",
      "Epoch: 20/20... Step: 2650... Loss: 1.2410... Val loss: 1.3009...\n",
      "Epoch: 20/20... Step: 2660... Loss: 1.2426... Val loss: 1.3011...\n",
      "Epoch: 20/20... Step: 2670... Loss: 1.2520... Val loss: 1.2979...\n",
      "Epoch: 20/20... Step: 2680... Loss: 1.2419... Val loss: 1.2992...\n",
      "Epoch: 20/20... Step: 2690... Loss: 1.2397... Val loss: 1.3008...\n",
      "Epoch: 20/20... Step: 2700... Loss: 1.2367... Val loss: 1.2972...\n",
      "Epoch: 20/20... Step: 2710... Loss: 1.2187... Val loss: 1.3005...\n",
      "Epoch: 20/20... Step: 2720... Loss: 1.2120... Val loss: 1.2973...\n",
      "Epoch: 20/20... Step: 2730... Loss: 1.2028... Val loss: 1.2959...\n",
      "Epoch: 20/20... Step: 2740... Loss: 1.2032... Val loss: 1.2987...\n",
      "Epoch: 20/20... Step: 2750... Loss: 1.2187... Val loss: 1.2971...\n",
      "Epoch: 20/20... Step: 2760... Loss: 1.2088... Val loss: 1.2969...\n",
      "Epoch: 20/20... Step: 2770... Loss: 1.2485... Val loss: 1.2923...\n",
      "Epoch: 20/20... Step: 2780... Loss: 1.2646... Val loss: 1.2910...\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 20\n",
    "\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'textgen-char-rnn_{}_{}.net'.format(dt.today().date(), int(dt.today().timestamp()))\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(f'./{model_name}', 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "    \"\"\"\n",
    "    Implements the prediction procedure for the character-level RNN\n",
    "    :param net: trained RNN for character-level prediction\n",
    "    :param char: character for which to predict the next character\n",
    "    :param h:\n",
    "    :param top_k:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # tensor inputs\n",
    "    x = np.array([[net.char2int[char]]])\n",
    "    x = one_hot_encoding(x, len(net.chars))\n",
    "    inputs = torch.from_numpy(x)\n",
    "\n",
    "    if train_on_gpu:\n",
    "        inputs = inputs.cuda()\n",
    "\n",
    "    # detach hidden state from history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    out, h = net(inputs, h)\n",
    "\n",
    "    # get character probs\n",
    "    p = F.softmax(out, dim=1).data\n",
    "    if train_on_gpu:\n",
    "        p = p.cpu()  # we're transferring p to CPU for further calculations\n",
    "\n",
    "    # get top k characters\n",
    "    if top_k is None:\n",
    "        top_ch = np.arange(len(net.chars))\n",
    "    else:\n",
    "        p, top_ch = p.topk(top_k)\n",
    "        top_ch = top_ch.numpy().squeeze()\n",
    "\n",
    "    p = p.numpy().squeeze()\n",
    "    char = np.random.choice(top_ch, p=p/p.sum())\n",
    "\n",
    "    return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "\n",
    "    if train_on_gpu:\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load checkpoint from saved model\n",
    "with open(os.getcwd() + '/' + model_name, 'rb') as f:\n",
    "    checkpoint = torch.load(f, map_location='cpu')\n",
    "\n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And Levin said, she was at the moment on the stees, when all to that the\n",
      "forming of inverent figure of standis of them\n",
      "had store to the best contention of that most\n",
      "simply of which he stroked at hard to see the most as too with the\n",
      "countess. Stepan Arkadyevitch was a smile of answers.\n",
      "\n",
      "He had to seen in\n",
      "her head of the presence of\n",
      "her husband's hand,\n",
      "her eyes thought of\n",
      "his, he had broken on, all to him to an explaining to him\n",
      "all again in the same town of the morning.\n",
      "\n",
      "\"You can be so many about you.\"\n",
      "\n",
      "\"I saw them to the same fact.... It\n",
      "seemed to him that he has such a sine than anything that he changed it into a portrait\n",
      "which we said that you\n",
      "were a significance. And we\n",
      "are never been for the sour of impressions; to\n",
      "supars is no tronse works, the secend secilliances and sighing. It's a minute of that mushace of happy, but I wanted to give herself, and we't so allosing it. But there's a line of a state of\n",
      "means, as he his foots one can see it with your heart, then you must say that he was the power of his face, but it's so much as there's no such a face of mind with him, and his feelings. He can't be\n",
      "standing, if I am standing for you to take it.\"\n",
      "\n",
      "\"Yes, it will be an implonation, I shall be, but tomarrissed it\n",
      "in the storm to her,\" said Stepan\n",
      "Arkadyevitch. \"Well, and share about the sight of that\n",
      "simply been bound at any of those way. That's not time for\n",
      "home to be and we did not know that they\n",
      "cannot companime? You can talk about him to mis change that I can't go into her. It's so deal to tell me that to say there is to get the convessation, here, it's\n",
      "officer, and told you, that im to say,\n",
      "and I did surpry. I cannot be\n",
      "already for the peasants, but I\n",
      "am a great mearow, but in a men in that satisfied\n",
      "with his sister's, and she would have told him to\n",
      "this thing in a little and always true to him. He's a letter at a\n",
      "little, and what he is in a singing at the country, who was no sense of mystendy\n",
      "than the setting of such stream to see him with that family truth. And that we mean\n"
     ]
    }
   ],
   "source": [
    "print(sample(loaded, 2000, top_k=5, prime='And Levin said'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
